# 推荐系统-基于模型协同过滤理论基础与业务实践

## 1.SparkMllib库框架详解

* Spark机器学习库
  * 五个组件
    * ML Algratham算法
    * Pipelines管道
    * Featureszation
    * Persistence
    * Utilitiesl
  * Sparkml和Sparkmllib
    * ml基于DatafrmaeAPI
    * mllib基于rdd的API

## 2.SparkMllib基本数据类型

* localvector本地向量

  * 创建方式上
    * dense稠密性向量---会存储0值和非0值
    * sparse稀疏性向量---仅可以存储非0值元素
      * seq()结构数据
      * 元素个数，下标，元素的值

* Lablepoint标签向量

  * 通过指定Vectors给定dense或sparse等向量

  * 从mllib.regression.LabeledPoint中获取labelpoint通过该方法给特征进行标签赋值

  * Spark读取libsvm格式数据

    * 鸢尾花-----花瓣的长度和宽度、花萼的长度和宽度

    * 鸢尾花几种类别---三种类别---setosa、versicolor、vernica

    * ```
      1 1:-0.555556 4:-0.916667 
      1 1:-0.666667 2:-0.166667 3:-0.864407 4:-0.916667 
      1 1:-0.777778 3:-0.898305 4:-0.916667 
      1 1:-0.833333 2:-0.0833334 3:-0.830508 4:-0.916667 
      1 1:-0.611111 2:0.333333 3:-0.864407 4:-0.916667 
      1 1:-0.388889 2:0.583333 3:-0.762712 4:-0.75 
      1 1:-0.833333 2:0.166667 3:-0.864407 4:-0.833333 
      1 1:-0.611111 2:0.166667 3:-0.830508 4:-0.916667 
      1 1:-0.944444 2:-0.25 3:-0.864407 4:-0.916667 
      1 1:-0.666667 2:-0.0833334 3:-0.830508 4:-1 
      1 1:-0.388889 2:0.416667 3:-0.830508 4:-0.916667 
      1 1:-0.722222 2:0.166667 3:-0.79661 4:-0.916667 
      1 1:-0.722222 2:-0.166667 3:-0.864407 4:-1 
      1 1:-1 2:-0.166667 3:-0.966102 4:-1 
      1 1:-0.166667 2:0.666667 3:-0.932203 4:-0.916667 
      1 1:-0.222222 2:1 3:-0.830508 4:-0.75 
      1 1:-0.388889 2:0.583333 3:-0.898305 4:-0.75 
      ```

    * libsvm格式非常适合于稀疏性数据(0值元素较多非0元素较少的情况)

    * ```
      (2.0列别标签,(9元素个数,[0,1,2]下标,[5.0,8.0,9.0]值))
      (1.0,(9,[0,1,2],[7.0,6.0,7.0]))
      (1.0,(9,[0,1,2],[3.0,2.0,1.0]))
      (2.0,(9,[0,1,2],[5.0,8.0,9.0]))
      ```

  * LocalMatrix--本地矩阵

    * ```scala
       val dm: Matrix = Matrices.dense(3, 2, Array(1.0, 3.0, 5.0, 2.0, 4.0, 6.0))
       println(dm(2,0))
       // Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))
       val sm: Matrix = Matrices.sparse(3, 2, Array(0, 1, 3), Array(0, 2, 1), Array(9, 6, 8))
       ```
      ```

    * 分布式矩阵(了解)
      ```

## 3.统计量的MLLIB实现

* 基于均值、方差、极差、分层抽样、假设检验等方面

## 4.Mllib抽取-转换-选择之特征提取器

* TF-IDF--适用于文本分析
* Word2Vec词向量工具----一个词能够使用不同词向量表示
* [[0.03173386193811894,0.009443491697311401,0.024377789348363876]]
* 建议自学word2vec原理
* CountVector--处理文本中的词频，按照词频进行原文本的排序

## 5.Mllib抽取-转换-选择之特征转换

* 二值化 

* stringtoindexer
* indextoString
* Bucketizer
* QuantileDiscretizer
* Sqltransformer

## 6.Mllib抽取-转换-选择之特征选择器

* 特征选择VectorSlicer
* RFormula根据R公式
* 卡方验证chisquare

## 6-1.最小二乘法

* 最小二乘法--数学上重要方法
* f(x)预测值 ---- y真实值
* 构建平方损失Loss=sum{(y-f(x))\**2}
* 求解lloss的导数，关于参数求导
* 令导数为0，得到参数求解的公式，利用公式迭代求解参数
* 最终得到参数最优解
* 和梯度下降法结合理解

## 7.ALS 实战从行为数据到评分再到预测-API简介

* ALS---交替最小二乘法
* 几个参数：
  * numblocks分块个数
  * rank隐因子个数
  * iterator迭代次数
  * regparrm正则化参数
  * implictPref显式反馈
  * alpha隐式反馈的参数---置信参数
  * nonnegative非负最小二乘法
* 显式反馈和隐式反馈
  * 用户有明确的喜好，如打分，显式反馈
  * 用户没有明确打分，但是有点击、浏览、收藏等行为，构成隐式反馈，通过alpha参数控制隐式反馈的参数
* 正则化参数
  * 主要通过正则化参数达到控制参数的复杂度，防止模型的过拟合
* 冷启动策略
  * 设置nan方式，但是会因为设置了nan评价参数业务nan
  * 设置drop方式，进一步在构建模型的时候不处理新加入的用户或商品
* SparkMl上调用Spark的ALS包
  * 基于dataframe
  * new ALS()添加参数
* SparkMLLIB上调用Spark的ALS包
  * 基于rdd的
  * ALS.train()

## 8.ALS 实战从行为数据到评分再到预测-需求分析与说明

* ALS实战电商数据
  * 数据来源：从大数据工程师处理好的结构化的数据给到算法工程师
  * 数据格式：[user,itemid,type,timestamp,times]
  * 对userid列来讲拿到的数据是需要处理的，采用stringIndexer将string类型的user转换为数值类型供计算

## 9.ALS 实战从行为数据到评分再到预测-实战

```scala
@Since("0.8.0")
case class Rating @Since("0.8.0") (
    @Since("0.8.0") user: Int,
    @Since("0.8.0") product: Int,
    @Since("0.8.0") rating: Double)
```

* 通过Spark处理数据成为Rating接受的类型---userid(int)+itemid(int)+rating(double)
* [user,itemid,type,timestamp,times]
* 处理user---->usercode---->userid----使用的方法是StringIndex(转化为datafrmame进行fit和transform)
* 处理type设置不同分数
  * pv浏览=1分
  * fav收藏=3分
  * buy购买=10分
* 设置rating打分规则
  * type数值化*times=得到得分
* 处理数据完毕
* 引入算法+数据=>模型
* SparkMllib中的ALS算法
* Als.train（numblocks,rank,regparam,iteration）
* predictions=als.predict
* rmse进行预测
* 输出推荐结果---uid+itemid+rating排序

## 10-ALS算法入门与LFM区别和联系

* LFM-Latent Factor machine隐因子分析模型
  * Y=A\*B
  * Y矩阵分解为两个矩阵的乘积的形式，通过随机初始化A矩阵和B矩阵，构建损失函数，利用梯度下降法，近似求解A和B矩阵的最优解
* ALS是在LFM基础上使用的交替的最下二乘法的方式进行参数的求解

## 11-ALS算法实战基础推荐

* 使用Scala完成协同过滤算法或余弦相似度的推荐(参考)
* ALS算法实战userid-itemid-rating得到模型的结果(理解)
* 参考文档中代码

## 12-ALS算法原理详解

* ALS算法利用交替最小二乘法最优解算法结合LFM的矩阵分解方式进行学习其中参数
* SVD将一个大的评分矩阵分解为了三个矩阵SVD(X)=U\*Sigma\*VT
  * 算法的时间复杂度较高的
  * SVD算法要求矩阵的数值是填充的
* A\*Y（Y.T\*Y）.inv=X

## 13-ALS算法步骤详解

* ALS算法思路：固定一个矩阵求解另外一个矩阵的导数，固定一个矩阵求解另外一个矩阵的导数
* ALS算法步骤：
  * 构建损失函数，Loss=(r-r_pred)\**2+lambda1\*u+lambda2\*v
  * 固定U矩阵求解V矩阵
    * v=M1.inv\*M2 其中M1和M2是关于U的矩阵的运算
  * 固定V矩阵求解U矩阵
    * u=M1.inv\*M2 其中M1和M2是关于V的矩阵的运算
  * 满足迭代次数停止

## 14-ALS算法显示反馈与隐式反馈

* 显式反馈直接通过显式打分进行矩阵分解
* 隐式反馈从数学层面讲，引入alpha置信参数，R是交互次数
* 定义隐式反馈的损失函数的参数：C=1+R\*alpha
* Loss=C\*(r-r_pred)\**2+lambda1\*u+lambda2\*v

## 15-ALS算法源码简介

* 了解ALS算法源码部分
* ALS算法并行化
  * numblocks---控制通信的复杂度和计算复杂度
  * makeblocks---inblock存储评分数据和outblock因子关联数据
  * inblock存储的格式[u1,u2,u3,u4]----[v1,v2,v3,v4]----[r1,r2,r3,r4]
  * outblock-----srtblock和Dstblockid映射关联数据

## 16.ALS推荐算法在Spark上的优化

* ALS优化场景
  * ALS加载数据
    * 使用Hadoop的 CombineFileInputFormat类进行小文件合并成split在加载到spark中，几块加载数据速度
  * ALS预测计算
    * 优化JVM中参数，效果不明显
    * 通过源码查看---看到了源码部分中ALS使用了笛卡尔积操作=====>指数级别(复杂度极高)
    * 在笛卡尔积的之前进行预处理分block块----420000块
    * 在预分块之前做一个预分区----加快数据分不到不同分区和block块下面进行处理的速度

## 17.SVD推荐算法简介

* 参考课件图示理解

## 18.PySpark基础环境搭建(了解)

* PySpark环境搭建
* 准备好大数据环境
  * jdk
  * hadoop2.7.4
  * spark2.3.3
  * 缺少组件
* 准备好Python环境
  * Anaconda
  * Python原生环境
* 结合形成Pyspark
  * 赋值spark源码包中的python目录下面的pyspark目录到lib下面的site-packages

## 19.构建PySpark简单推荐系统(了解)

* 通过pyspark构建推荐系统
* 使用python数据科学包有一个pandas的包进行数据科学开发
* 使用pyspark完成简单推荐系统
  * 使用pandas处理数据格式为-userid+itemid+rating打分
  * 使用pyspark中als算法进行训练模型
  * 使用模型预测5个喜欢的歌手
  * 将喜欢的歌手id和歌手名字进行关联进行输出

## 20.总结

* 查看XMIND









