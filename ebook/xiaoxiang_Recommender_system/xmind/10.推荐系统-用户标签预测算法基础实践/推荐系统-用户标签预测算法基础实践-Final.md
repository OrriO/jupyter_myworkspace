## 推荐系统-用户标签预测算法基础实践

## 1.泰坦尼克号获救人员识别实战

* 加强iris的代码实战(掌握)
* 泰坦尼克号的问题
  * 加载数据--pandas和自身的数据
  * 预处理
  * 特征工程---类别型数据的处理
  * 建立模型
  * 模型预测
  * 模型校验

## 2.线性回归理论基础及算法详解

* 线性回归----监督学习的连续值预测问题

* 回归分为几类：

  * 简单线性回归

    * y=kx+b
    * 目的：求解K和b

  * 多元线性回归

    * y=w0+w1x1+w2x2.。。。。
    * 目的：求解w0，w1，w2

  * 如何求解最优解参数？

    * 利用平方损失(经验风险)求解参数
    * 求解导数，令导数为0，求解驻点
    * 利用梯度下降法结合导数进行学习
    * X（k+1）=X(k)-alpha\*df（梯度算子）

    ​

## 3.线性回归房价预测实战

* 汽车销量预测
  * 利用pandas读取和处理数据
  * 特征工程
  * 建立线性回归模型
  * 预测
* 房价预测
  * 从sklearn中加载数据
  * 读取数据集中关键信息
  * data和target
  * 切分数据
  * 训练模型
  * 模型预测
  * 模型校验“mae、mse、r2
* Lasso回归：线性回归损失函数+L1正则
* Ridge回归：线性回归损失函数+L2正则
* Elasticnet回归：线性回归损失函数+L1正则+L2正则

## 4.Cart回归树原理详解

* Cart树
  * 特征选择
    * Cart树特征选择依赖于Gini系数
  * 决策树生成
    * Cart树算法----二叉树
  * 决策树剪枝
    * 后剪枝
* Cart树的回归树：
  * 回归树是二叉树
  * 寻找二分节点的特征以及特征的取值---寻找切分点以及切分变量
  * 算法步骤：
    * 1.选择最优切分点和切分变量，j和s达到最优
    * 2.使用选择的j和s划分区域，给出最终的输出值
    * 3.递归调用上述的子区域中寻找最优切分点以及切分变量，
    * 4.将输入空间划分为M个区域，生成决策树

## 5.Cart分类树原理简介

* gini系数：
  * Gini系数公式
    * Gini=sum(pi(1-pi))
    * Gini=1-sum(pi**2)
  * 集合的Gini系数
    * D1/|D|*Gini1+D2/|D|\*Gini2


* Cart树分类树：
  * 分类树是二叉树，衡量指标是gini系数
  * Cart树思想：
    * 首先选择最佳切分点以及最佳切分变量
    * 其次，求解集合的Gini系数，在所有的特征以及取值中获取到Gini系数最小的特征以及特征值
    * 其次，递归调用Cart树产生的方法，直到满足停止迭代的条件为止
    * 最后得到，分类树
* Cart树例子：
  * Cart树是分类和回归树
  * 基于gini系数进行分类----计算集合的Gini系数
  * 求解最小的Gini系数的值作为最优切分点和切分变量
  * 如果仍然有样本点没有被分类，继续划分，寻找下一个划分点以及划分变量

## 6.基尼系数

## 7.Cart树剪枝

* 树剪枝--先剪枝和后剪枝
* Cart树采用的是后剪枝
* 采用剪枝系数来确定如何剪枝？---得到最小的a值

## 8.Cart树实战回归和分类问题

* 代码参考具体
* Cart树区别于ID3和C4.5区别
  * 二叉树
  * Gini系数
  * 在回归树上无差别---mse和mae

## 9.随机森林简介及集成学习基础

* 随机森林
  * 随机：构建森林需要多样化的----如何随机？怎么随机？
  * 森林：由大于等于2颗树组成的森林
* 集成学习算法
  * 并行学习
    * 随机森林
    * Bagging算法
  * 串行学习
    * Boosting
    * Adaboost算法
    * GBDT算法
    * XGBOOST算法

## 10.随机森林算法详解

* 随机森林算法-random-forest
* 算法思想：根据样本有放回的抽样，N次抽样得到N个样本，组成数据子集，在根据特征或属性的抽样得到不同的列特征，进而组合为新的数据子集，通过gini系数或entropy等参数确定每颗决策树，从而根据随机森林表决得到最终结果
* 算法步骤：
  * 1.将所有的样本有放回抽样，得到抽样后的样本子集
  * 2.在继续对特征或属性列进行抽样，log2d特征，组成数据子集
  * 3.将不同的数据子集结合机器学习算法构建不同的机器学习模型，构建随机森林模型
  * 4.通过随机森林中每颗决策树得到预测结果，根据少数服从多数的原则进而对数据进行分类。
* 算法改进：
  * XGBOOST中结合并行学习和串行学习的方式，特别是借鉴了随机森林对行or列进行随机的特征
* 随机森林随机性体现在哪里？
  * 1.样本的随机性------Bagging算法中提到
  * 2.属性随机性--------log2d、sqrt(d)

## 11.随机森林算法分类及回归实战

* skleran的随机森林
  * 分类-RandomForestClassifier----gini
  * 回归-RandomForestRegressor----mse

## 12.集成学习算法注意事项

* 集成学习中并行学习方式需要进行投票表决
  * 三种方式
* 集成学习结果并不一定会提高样本的学习的结果
  * 参考课件

## 13.Bagging算法原理

* 基于Boostrip抽样----自助采样
  * 结果：63.2%比例被抽样，36.8%比例不被抽样
* Bagging算法：
  * 算法思想：首先采用基于bootstrip的样本的抽样得到T个数据的子集，由每一个数据的子集训练得到模型，通过模型得到多个预测结果，通过少数服从多数的原则进行最终分类。
  * 算法原理：
    * 1.基于Bootstrip抽样进行样本抽样，得到抽样的T个集合
    * 2.将T个数据子集通过不同的机器学习算法形成机器学习模型
    * 3.通过机器学习模型得到预测结果，对预测结果进行统计得到出现次数最多的分类结果
  * 算法改进：
    * 随机森林(增加属性抽样)

## 14.Bagging算法实战

* Bagging算法参数
  * estimator参数指定学习器--knn,默认的是决策树
    * bootstrip抽样
    * bootstrip_features抽样
* Bagging分类和回归
  * BaggingClassifier
  * BaggingRegressor

## 15.Boosting算法原理

* Boosting---串行学习方法
* Boosting框架：
  * 1.训练基学习器
  * 2.根据第一个基学习器对分对或分错的样本进行加权
  * 3.根据各个分类器效果对不同的学习器进行加权，学习器的加权

## 17.Adaboost算法原理

* Adaboost算法--自适应增强算法
  * 样本权重调整
  * 分类器权重调整
* Adaboost算法步骤：
  * 1.对数据集中N个样本进行权值的初始化，初始化为1/N
  * 2.训练基分类器对于分错的样本增加他的权重，分对的样本降低它的权重
  * 3.调整分类器的权重，得到最终的分类器
* Adaboost数学原理实战：
  * 首先Adaboost算法是解决二分类问题
  * {-1，+1}两个类别
  * 对样本进行初始化权值，1/N
  * 定义了hm(x)模型本身---KNN\DT
  * 定义em分类误差率----sum(w\*I(hi==hi(x)))
  * 定义am学习器的权值----am=1/2\*log({1-em}/em)
  * 调整权值----w(k+1)=w(k)\*exp(-am\*yi\*hm(x))/zm
  * 得到结果：f(x)=sum(am\*hm(x))
  * 二分类输出：F(x)=sign(f(x))

## 18.Adaboost算法实战

* Adaboost算法损失函数---指数损失
* Adaboost算法案例(掌握)
* Adaboost算法的多分类问题求解--ovo--ovr---MVM
* Adaboost实战
  * Adaboost参数信息
    * 1.学习器
    * 2.学习率
    * 3.多少串行学习器

## 19.Adaboost实战葡萄酒数据

* 对于数据的处理，需要增加列明子--wine_names取
* 训练模型和前面部分一致的

## 20.GBDT算法详解

* GBDT模型
  * GBDT梯度提升决策树
  * 提升树原理
    * 加法模型：最终的结果依赖于每一个模型
    * 前向分布算法：通过该算法求解和迭代模型的参数
  * GBDT在提升树的原理基础上利用梯度的方法进行最优解参数的求解

## 21.GBDT算法改进的XGBOOST算法

* GBDT
  * 加法模型
  * 前向分布算法
  * 梯度提升算法
* XGBOOST
  * 泰勒二阶展开
  * L1和L2正则---叶子结点个数和权重
  * 不需要对缺失数据进行处理
  * level-wise
* LightGBM
  * leaf-wise
  * 不需要对类别型变量进行处理

## 22.集成模型回归分析波斯顿房价实战

* 房价预测---回归问题
* 决策树的预测分析DescitionTree
* 随机森林预测分析RandomForestRegressor
* Bagging算法预测分析：BaggingRegressor
* Adaboost算法预测分析：AdaboostRegressor
* GBDT算法预测分析：GradientBoostingRegressor

## 23.集成学习如何保证多样性？

* 数据样本的多样性
* 特征或属性的多样性
* 算法参数的多样性

## 24.总结

* 集成学习
  * 串行----Boosting框架-Adaboost
  * 并行----Bagging框架--RandomForest
  * Stacking组合方式







